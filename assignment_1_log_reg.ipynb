{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df66261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, plot_roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dc42871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../20220911_data.csv')\n",
    "df.drop(columns=['hire_date'], inplace=True)\n",
    "for var in ('gender', 'ethnicity'):\n",
    "    temp = pd.get_dummies(df[var], prefix=var, drop_first=True)\n",
    "    df.drop(columns=[var], inplace=True)\n",
    "    df = df.join(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55a8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4d746fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['terminated_in_first_year']\n",
    "X = df.drop(columns=['terminated_in_first_year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "44be28f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X.columns.str.contains('strengths|weakness|compared_to_others')\n",
    "\n",
    "# TODO: scale to 0-1\n",
    "crosschq_vars = X.iloc[:, cols].values\n",
    "crosschq_vars = MinMaxScaler().fit_transform(crosschq_vars)\n",
    "\n",
    "pca = PCA(n_components=crosschq_vars.shape[1])\n",
    "pca.fit(crosschq_vars)\n",
    "\n",
    "var_explained = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "components = np.argmax(var_explained >= 0.75)\n",
    "\n",
    "pca75 = PCA(components)\n",
    "crosschq_vars75 = pca75.fit_transform(crosschq_vars)\n",
    "\n",
    "X = X.iloc[:, ~cols]\n",
    "crosschq_vars75 = pd.DataFrame(crosschq_vars75)\n",
    "crosschq_vars75.columns = [f'pcomponent_{i}' for i in crosschq_vars75.columns]\n",
    "X = X.join(crosschq_vars75)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "69ff0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "70b8e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = torch.Tensor(X_train.astype(float).values),torch.Tensor(X_test.astype(float).values)\n",
    "y_train, y_test = torch.Tensor(y_train.astype(float).values),torch.Tensor(y_test.astype(float).values)\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5f6a8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X.shape[1]\n",
    "hidden_dim = 16 # param\n",
    "output_dim = 1\n",
    "dropout_rate = 0.1 # param\n",
    "learning_rate = 0.01 # param\n",
    "momentum = 0.9 # param\n",
    "\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.cf1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.cf2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.cf3 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.cf1(x))\n",
    "        x = torch.sigmoid(self.cf2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.cf3(x))\n",
    "        return x\n",
    "    \n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "db4cb8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0] loss: 0.001 ||| test accuracy: 0.805\n",
      "[  500] loss: 0.001 ||| test accuracy: 0.827\n",
      "[ 1000] loss: 0.001 ||| test accuracy: 0.830\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [177], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/assignment_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [157], line 18\u001b[0m, in \u001b[0;36mLogisticRegression.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcf1(x))\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcf2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcf3(x))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/assignment_1/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/assignment_1/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(10000):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs = X_train\n",
    "    labels = y_train\n",
    "\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'[{epoch:5d}] loss: {(running_loss/500):.3f} ||| test accuracy: {((model(X_test).round() == y_test).sum()/len(y_test)):.3f}')\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "db91e4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7953)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "(model(X_train).round() == y_train).sum()/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "84f29e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8207)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model(X_test).round() == y_test).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3aaf4eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8207)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model(X_test).round() == y_test).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c572547c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, tensor(0.4675, grad_fn=<BinaryCrossEntropyBackward0>))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_loss, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9b2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0af0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
